# sentiment_analyzer.py

import os
import json
import logging
import hashlib
from typing import List, Dict, Optional
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
import time

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class SentimentAnalyzer:
    """GPT ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„"""
    
    def __init__(self, batch_size: int = 20):
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.batch_size = batch_size
        self.cache_file = Path("data/sentiment_cache.json")
        self.cache = self._load_cache()
        self.logger = logging.getLogger(__name__)
        
        # í†µê³„
        self.stats = {
            'api_calls': 0,
            'cache_hits': 0,
            'total_analyzed': 0
        }
    
    def _load_cache(self) -> Dict:
        """ìºì‹œ ë¡œë“œ"""
        if self.cache_file.exists():
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
    def _get_cache_key(self, headline: str) -> str:
        """í—¤ë“œë¼ì¸ì˜ í•´ì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(headline.encode()).hexdigest()
    
    def _create_prompt(self, headlines: List[str]) -> str:
        """ë°°ì¹˜ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        headlines_json = json.dumps(headlines, ensure_ascii=False)
        
        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ë“¤ì— ëŒ€í•´ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

í—¤ë“œë¼ì¸ ëª©ë¡:
{headlines_json}

ê° í—¤ë“œë¼ì¸ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

1. sentiment: "positive" (ê¸ì •), "negative" (ë¶€ì •), "neutral" (ì¤‘ë¦½)
2. score: 0.0~1.0 ì‚¬ì´ ì ìˆ˜ (0=ë§¤ìš° ë¶€ì •, 0.5=ì¤‘ë¦½, 1.0=ë§¤ìš° ê¸ì •)
3. confidence: 0.0~1.0 ì‚¬ì´ ì‹ ë¢°ë„
4. reasoning: ê°„ë‹¨í•œ ë¶„ì„ ì´ìœ  (20ì ì´ë‚´)

ë¶„ì„ ê¸°ì¤€:
- ì‹¤ì  í˜¸ì¡°, ìˆ˜ìµ ì¦ê°€, ì„±ì¥ ë“± â†’ ê¸ì •
- ì‹¤ì  ë¶€ì§„, ì†ì‹¤, í•˜ë½ ë“± â†’ ë¶€ì •
- ë‹¨ìˆœ ì‚¬ì‹¤ ì „ë‹¬, ì¤‘ë¦½ì  í‘œí˜„ â†’ ì¤‘ë¦½

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì˜ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í¬í•¨ ê¸ˆì§€):
[
  {{
    "headline": "ì›ë³¸ í—¤ë“œë¼ì¸",
    "sentiment": "positive",
    "score": 0.75,
    "confidence": 0.85,
    "reasoning": "ë¶„ì„ ì´ìœ "
  }},
  ...
]

**ì¤‘ìš”: JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ê³ , ```jsonì´ë‚˜ ë‹¤ë¥¸ ë§ˆí¬ë‹¤ìš´ í¬ë§·ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**"""
        
        return prompt
    
    def _parse_response(self, response_text: str) -> List[Dict]:
        """GPT ì‘ë‹µ íŒŒì‹±"""
        try:
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            text = response_text.strip()
            if text.startswith('```'):
                text = text.split('```')[1]
                if text.startswith('json'):
                    text = text[4:]
            text = text.strip()
            
            # JSON íŒŒì‹±
            results = json.loads(text)
            return results
            
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.logger.debug(f"ì›ë³¸ ì‘ë‹µ: {response_text}")
            return []
    
    def analyze_batch(self, headlines: List[str]) -> List[Dict]:
        """í—¤ë“œë¼ì¸ ë°°ì¹˜ ë¶„ì„"""
        if not headlines:
            return []
        
        self.logger.info(f"ğŸ“Š Analyzing batch of {len(headlines)} headlines...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_prompt(headlines)
        
        # API í˜¸ì¶œ
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                max_tokens=2000
            )
            
            self.stats['api_calls'] += 1
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content or ""
            
            if not response_text:
                self.logger.error("API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return []
            
            results = self._parse_response(response_text)
            
            self.logger.info(f"âœ… Analyzed {len(results)} headlines")
            return results
            
        except Exception as e:
            self.logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_headlines(self, headlines: List[str]) -> List[Dict]:
        """í—¤ë“œë¼ì¸ ë¦¬ìŠ¤íŠ¸ ì „ì²´ ë¶„ì„ (ìºì‹± + ë°°ì¹˜)"""
        results = []
        to_analyze = []
        cached_results = {}
        
        # 1. ìºì‹œ í™•ì¸
        for headline in headlines:
            cache_key = self._get_cache_key(headline)
            if cache_key in self.cache:
                cached_results[headline] = self.cache[cache_key]
                self.stats['cache_hits'] += 1
            else:
                to_analyze.append(headline)
        
        self.logger.info(f"ğŸ’¾ Cache: {len(cached_results)} hits, {len(to_analyze)} to analyze")
        
        # 2. ë°°ì¹˜ ë¶„ì„
        batch_results = {}
        for i in range(0, len(to_analyze), self.batch_size):
            batch = to_analyze[i:i + self.batch_size]
            
            self.logger.info(f"ğŸ”„ Processing batch {i//self.batch_size + 1}/{(len(to_analyze)-1)//self.batch_size + 1}")
            
            batch_analysis = self.analyze_batch(batch)
            
            # ê²°ê³¼ ë§¤í•‘
            for item in batch_analysis:
                headline = item.get('headline', '')
                if headline in batch:
                    batch_results[headline] = item
                    
                    # ìºì‹œì— ì €ì¥
                    cache_key = self._get_cache_key(headline)
                    self.cache[cache_key] = item
            
            # API í˜¸ì¶œ ê°„ ë”œë ˆì´ (Rate limit ë°©ì§€)
            if i + self.batch_size < len(to_analyze):
                time.sleep(1)
        
        # 3. ê²°ê³¼ ë³‘í•© (ì›ë³¸ ìˆœì„œ ìœ ì§€)
        for headline in headlines:
            if headline in cached_results:
                results.append(cached_results[headline])
            elif headline in batch_results:
                results.append(batch_results[headline])
            else:
                # ë¶„ì„ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’
                results.append({
                    'headline': headline,
                    'sentiment': 'neutral',
                    'score': 0.5,
                    'confidence': 0.0,
                    'reasoning': 'ë¶„ì„ ì‹¤íŒ¨'
                })
        
        # 4. ìºì‹œ ì €ì¥
        self._save_cache()
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_analyzed'] = len(results)
        
        return results
    
    def get_statistics(self) -> Dict:
        """ë¶„ì„ í†µê³„"""
        total = self.stats['total_analyzed']
        cache_rate = (self.stats['cache_hits'] / total * 100) if total > 0 else 0
        
        return {
            'total_analyzed': total,
            'api_calls': self.stats['api_calls'],
            'cache_hits': self.stats['cache_hits'],
            'cache_rate': f"{cache_rate:.1f}%",
            'estimated_cost': f"${self.stats['api_calls'] * 0.01:.2f}"  # ëŒ€ëµì  ì¶”ì •
        }


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    analyzer = SentimentAnalyzer(batch_size=20)
    
    # í…ŒìŠ¤íŠ¸ í—¤ë“œë¼ì¸
    test_headlines = [
        "ì‚¼ì„±ì „ì, 3ë¶„ê¸° ì˜ì—…ì´ìµ 10ì¡°ì› ëŒíŒŒ...ì‹œì¥ ì˜ˆìƒ ìƒíšŒ",
        "SKí•˜ì´ë‹‰ìŠ¤, ë°˜ë„ì²´ ìˆ˜ì¶œ ê¸‰ê°ìœ¼ë¡œ ì‹¤ì  ì•…í™” ìš°ë ¤",
        "ë„¤ì´ë²„, AI ì±—ë´‡ ì„œë¹„ìŠ¤ 'í•˜ì´í¼í´ë¡œë°”X' ì •ì‹ ì¶œì‹œ",
        "í˜„ëŒ€ì°¨Â·ê¸°ì•„, ë¯¸êµ­ ì „ê¸°ì°¨ íŒë§¤ í˜¸ì¡° ì§€ì†",
        "LGì—ë„ˆì§€ì†”ë£¨ì…˜, ë¶ë¯¸ ë°°í„°ë¦¬ ê³µì¥ ê°€ë™ë¥  ì €ì¡°",
        "ì¹´ì¹´ì˜¤, ë©”ì‹ ì € ê´‘ê³  ë§¤ì¶œ ì¦ê°€ì„¸",
        "ì…€íŠ¸ë¦¬ì˜¨, ë°”ì´ì˜¤ì‹œë°€ëŸ¬ ìŠ¹ì¸ ì§€ì—°ìœ¼ë¡œ ì£¼ê°€ í•˜ë½",
        "POSCO, ì² ê°• ê°€ê²© ìƒìŠ¹ìœ¼ë¡œ ìˆ˜ìµì„± ê°œì„ ",
    ]
    
    # ë¶„ì„
    print("\nğŸ” ê°ì„± ë¶„ì„ ì‹œì‘...")
    print("=" * 60)
    
    results = analyzer.analyze_headlines(test_headlines)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print("=" * 60)
    for result in results:
        sentiment_emoji = {
            'positive': 'ğŸ˜Š',
            'negative': 'ğŸ˜',
            'neutral': 'ğŸ˜'
        }
        
        emoji = sentiment_emoji.get(result['sentiment'], 'â“')
        print(f"\n{emoji} {result['sentiment'].upper()} (ì ìˆ˜: {result['score']:.2f})")
        print(f"í—¤ë“œë¼ì¸: {result['headline']}")
        print(f"ì´ìœ : {result['reasoning']}")
        print(f"ì‹ ë¢°ë„: {result['confidence']:.2f}")
    
    # í†µê³„
    print("\n\nğŸ“ˆ í†µê³„:")
    print("=" * 60)
    stats = analyzer.get_statistics()
    for key, value in stats.items():
        print(f"{key}: {value}")