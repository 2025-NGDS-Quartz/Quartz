# news_pipeline.py (ì™„ì „ ë²„ì „)

import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

from stock_matcher import StockMatcher
from sentiment.sentiment_analyzer import SentimentAnalyzer
from stock_aggregator import StockAggregator

class NewsPipeline:
    """ë‰´ìŠ¤ í¬ë¡¤ë§ â†’ ì¢…ëª© ë§¤ì¹­ â†’ ê°ì„± ë¶„ì„ â†’ ì¢…ëª©ë³„ ì§‘ê³„ í†µí•© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.setup_logging()
        self.stock_matcher = StockMatcher()
        self.sentiment_analyzer = SentimentAnalyzer(batch_size=20)
        self.aggregator = StockAggregator()
        self.logger = logging.getLogger(__name__)
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        )
    
    def load_news_file(self, filepath: str) -> List[Dict]:
        """í¬ë¡¤ë§ëœ ë‰´ìŠ¤ íŒŒì¼ ë¡œë“œ"""
        self.logger.info(f"ğŸ“‚ Loading news from {filepath}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        news_items = data.get('news', [])
        self.logger.info(f"âœ… Loaded {len(news_items)} news items")
        
        return news_items
    
    def process_news(self, news_items: List[Dict]) -> List[Dict]:
        """ë‰´ìŠ¤ ì²˜ë¦¬: ì¢…ëª© ë§¤ì¹­ + ê°ì„± ë¶„ì„"""
        
        # 1. ì¢…ëª© ë§¤ì¹­
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ” Step 1: Stock Matching")
        self.logger.info("="*60)
        
        news_items = self.stock_matcher.add_tickers_to_news(news_items)
        
        matched_count = sum(1 for item in news_items if item.get('tickers'))
        self.logger.info(f"âœ… Matched {matched_count}/{len(news_items)} news to stocks")
        
        # 2. ê°ì„± ë¶„ì„
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ˜Š Step 2: Sentiment Analysis")
        self.logger.info("="*60)
        
        # í—¤ë“œë¼ì¸ ì¶”ì¶œ
        headlines = [item['headline'] for item in news_items]
        
        # ê°ì„± ë¶„ì„
        sentiment_results = self.sentiment_analyzer.analyze_headlines(headlines)
        
        # ê²°ê³¼ ë³‘í•©
        for i, item in enumerate(news_items):
            if i < len(sentiment_results):
                sentiment = sentiment_results[i]
                item['sentiment'] = sentiment.get('sentiment', 'neutral')
                item['sentiment_score'] = sentiment.get('score', 0.5)
                item['sentiment_confidence'] = sentiment.get('confidence', 0.0)
                item['sentiment_reasoning'] = sentiment.get('reasoning', '')
        
        # í†µê³„
        stats = self.sentiment_analyzer.get_statistics()
        self.logger.info(f"\nğŸ“Š Sentiment Analysis Statistics:")
        for key, value in stats.items():
            self.logger.info(f"  {key}: {value}")
        
        return news_items
    
    def save_processed_news(self, news_items: List[Dict], output_file: Optional[str] = None) -> str:
        """ì²˜ë¦¬ëœ ë‰´ìŠ¤ ì €ì¥"""
        output_dir = Path("./data/processed")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if not output_file:
            now = datetime.now()
            output_file = now.strftime("processed_%Y%m%d_%H.json")
        
        filepath = output_dir / output_file
        
        # í†µê³„ ê³„ì‚°
        total = len(news_items)
        with_tickers = sum(1 for item in news_items if item.get('tickers'))
        sentiment_dist = {
            'positive': sum(1 for item in news_items if item.get('sentiment') == 'positive'),
            'negative': sum(1 for item in news_items if item.get('sentiment') == 'negative'),
            'neutral': sum(1 for item in news_items if item.get('sentiment') == 'neutral')
        }
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'total_count': total,
            'statistics': {
                'news_with_stocks': with_tickers,
                'stock_match_rate': f"{with_tickers/total*100:.1f}%" if total > 0 else "0%",
                'sentiment_distribution': sentiment_dist
            },
            'news': news_items
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"\nğŸ’¾ Saved processed news to {filepath}")
        return str(filepath)
    
    def run(self, input_file: str, output_file: Optional[str] = None) -> str:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("\n" + "ğŸš€ "*30)
        self.logger.info("NEWS PROCESSING PIPELINE START")
        self.logger.info("ğŸš€ "*30)
        
        start_time = datetime.now()
        
        try:
            # 1. ë‰´ìŠ¤ ë¡œë“œ
            news_items = self.load_news_file(input_file)
            
            # 2. ì²˜ë¦¬ (ì¢…ëª© ë§¤ì¹­ + ê°ì„± ë¶„ì„)
            processed_news = self.process_news(news_items)
            
            # 3. ì €ì¥
            output_path = self.save_processed_news(processed_news, output_file)
            
            # âœ… 4. ì¢…ëª©ë³„ ì§‘ê³„ (ìƒˆë¡œ ì¶”ê°€)
            self.logger.info("\n" + "="*60)
            self.logger.info("ğŸ“Š Step 3: Stock Aggregation")
            self.logger.info("="*60)
            
            aggregated = self.aggregator.aggregate_by_stock(processed_news)
            candidate_path = self.aggregator.save_candidates(aggregated)
            
            # ìƒìœ„ 10ê°œ ì¶œë ¥
            self.aggregator.print_summary(aggregated, top_n=10)
            
            # ì™„ë£Œ
            elapsed = (datetime.now() - start_time).total_seconds()
            
            self.logger.info("\n" + "âœ… "*30)
            self.logger.info(f"PIPELINE COMPLETED in {elapsed:.2f}s")
            self.logger.info("âœ… "*30)
            self.logger.info(f"\nğŸ“ Outputs:")
            self.logger.info(f"  - Processed news: {output_path}")
            self.logger.info(f"  - Stock candidates: {candidate_path}")
            
            return output_path
            
        except Exception as e:
            self.logger.error(f"âŒ Pipeline failed: {e}", exc_info=True)
            raise


def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    pipeline = NewsPipeline()
    
    # ê°€ì¥ ìµœê·¼ í¬ë¡¤ë§ íŒŒì¼ ì°¾ê¸°
    news_dir = Path("./data/news_raw")
    news_files = sorted(news_dir.glob("*.json"), reverse=True)
    
    if not news_files:
        print("âŒ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € multi_news_crawler.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # ìµœì‹  íŒŒì¼ ì‚¬ìš©
    latest_file = news_files[0]
    print(f"\nğŸ“ Processing file: {latest_file.name}")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    output_path = pipeline.run(str(latest_file))
    
    print(f"\nâœ… Done!")


if __name__ == "__main__":
    main()